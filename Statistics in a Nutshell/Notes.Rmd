---
title: "Statistics in a Nutshell Notes"
author: "Scott Brenstuhl"
output: html_document
---
```{r}
library(htmlTable)
library(knitr)
```

## Basic Concepts

### Types of Data

Nominal data: Numbers function as a name or label but do not have a numeric
meaning.

- Binary data: Can only be true or false.
    
Ordinal data: Data that has a meaningful order, but their is no metric that acts
as a scale to quantify the distance between catagories. It is appropriate to 
calculate the medium but not the mean.

Interval data: Has meaningful order and equal intervals between measurments
represent equal change, but has no natural zero point. These are rare 
(Fahrenheit is the common example). Multiplication and devision are not 
appropriate.

Ratio data: Has natural order, equal intervals, and a natural zero point. This
includes many physical measurments (height, weight, age). 

Continuous data: Can take any value within a range. Aka measurments can be 
percise such as fractions of an inch, adding days and hours to your age etc.

Discrete data: Can only take on particular values, such as anything that is 
being counted. Nominal, binary, and rand ordered data are all discrete.


### Error

True and Error Scores:

> X(measurment) = T(true score) + E(error)

Random error: Due to chance, no particular pattern and will cancel itself out
over repeted measure. Must be unrelated to the true score and correlation 
between errors must be zero.

Systematic error: Has an observable pattern not due to chance, and offten has
a cause that can be identified and remedied.

### Reliability

Reliability: How consistent or repeatable measurments are.

Multiple-occasion reliability: How similarly a test or scale perfoems over 
repeted testings.

Multiple-forms reliability: How similarly different version of a test perform in
measuring the same entity.

Internal consistency reliability: How much the items on a test are measuring the
same thing.

Measures of agreement: Appopriate for judging consistency when measurment 
problems have categorical judgements (such as true/false).

Percent agreement (The simplest measure of agreement): 

> P(Percent agreement) = A(Cases raters agree) / T(Total number of ratings)

kappa (Cohen's kappa): Preferable to percent agreement because it is corrected
for agreement due to chance. Kappa is always less than or equal to percent 
agreement due to it being adjusted for chance agreement.

```{r echo=FALSE}
kappa_table <- data.frame("Pos" = c("a","c"), "Neg" = c("b", "d"))
row.names(kappa_table) <- c("Pos", "Neg")

kappa_table
```

> _k_ = P~0~(Observed agreement) - P~e~(Expected agreement) / 1 - P~e~

> P~0~ = (a + d)/(a + b + c + d)

> x = ((a + c) * (a + b)) / (a + b + c + d)
> y = ((d + c) * (d + b)) / (a + b + c + d)
> P~e~ = (x + y) / (a + b + c + d)

Kappa has a range of 0-1 with 0 being if observed agrement is the same as 
random agreement and 1 is all cases are in agreement:

< 0 Poor
0 - .2 Slight
.21 - .4 Fair
.41 - .6 Moderate
.61 - .8 Substantial
.81 - 1 Almost perfect


```{r, echo=FALSE}

```

